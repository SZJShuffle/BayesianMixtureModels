# 课程简介

> 这门课是我最近上完的一门课，完成后感觉收获挺多的。本文主要是我上完课后对课程一个简要的总结，供参考。

课程名称：Bayesian Statistics: Mixture Models

课程链接：https://www.coursera.org/learn/mixture-models

开设单位：UCSC

这门课好像是最近几个月才开的一门新课，相比于coursera上的其他课程，它的关注量不是很高。课程主要介绍的是(贝叶斯)混合生成模型（(bayesian) generative mixture models）及其参数推断(EM/MCMC,不涉及变分推断)的方法。严格来说，它是机器学习里面的一个小的分枝。

这是一门原理和实践相结合的课程，主要以高斯混合/零膨胀混合等模型为实例，讲解其原理和推导，并辅以演示代码帮助理解；同时也留有相应的作业帮助读者更好地理解相关概念。作业如果认真完成的话，这门课程涉及的知识点应该是大多都能掌握了。

课程难度对于我这种劝退学科出身的人来说适中，按照coursera的定位是advanced，官方估计完成时间大约21小时左右。当然要求一些预备知识。

## **前置知识**

根据我的学习体会，以下知识点是需要的：

1.基础的概率论。主要是各种概率分布，如正态/指数/泊松等；特别地，贝叶斯统计中常见的几种分布，如：beta/dirichlet/gamma等最好有所了解。

2.一点线性代数。课程中会涉及到一些multivariate分布相关的推导，比如：Multivariate Gaussian/Wishart分布等。理解这一部分要求对矩阵的运算有所了解。

3.基本的R语言。实例演示和作业提交都是基于R的。因此对其常规语法、数据结构的基本了解是需要的。

4.一点机器学习（非必须）。课程里面涉及到的EM和MCMC部分只会讲算法的运行流程，但是原理则涉及的比较少。比如：EM课程只会告诉你E步和M步是什么；Gibbs sampling只会告诉你它是基于fully conditional分布进行采样的，但是为什么是这样则没有介绍。读者如果在此前对这些算法有所了解的话，学习效果会更好。此外，一些机器学习里面的基本术语是需要了解的。

5.一点随机过程（非必须）。主要是对马氏过程的性质有一定的了解，例如：常返/暂态/细节平衡等。相信如果对马氏过程有所了解的话，课程内涉及到的MCMC理解起来不成问题。

## **课程大纲**

第一章:

主要介绍混合模型的基本概念。包括：基本定义与性质/其完全、不完全形式的似然函数。

第二章:

主要介绍如何利用EM推断混合模型的参数，同时会有一些实例以及演示代码：高斯混合模型/零膨胀模型。

第三章:

本章终于开始涉及贝叶斯了，主要介绍当混合模型加上先验以后，如何利用MCMC进行参数推断。涉及到的例子包括Univariate/Multivariate gaussian mixture/Zero-inflated-Poisson/Expo-logGaussian mixture。

第四章:

主要介绍混合模型的用途：包括无监督聚类/密度估计/(半)监督分类。在半监督分类里面，会和一些经典的分类器如：线性判别/二次型判别/高斯朴素贝叶斯等做对比。

第五章:

主要介绍一些运用混合模型时可能用到的技巧与问题。包括如何在计算时使数值稳定/随机初始化可能带来的问题/模型选择等等。

## **学习建议**

1.由于课程在算法原理的推导上涉及比较少，因此如果对EM和MCMC不怎么了解的话，推荐参考李航的《统计学习》第二版（第一版没有），里面有详细的介绍。

2.涉及到贝叶斯统计的模型，个人体会最好是结合概率图来理解，这一部分推荐参考《PRML》的第八章，讲的非常详细。

